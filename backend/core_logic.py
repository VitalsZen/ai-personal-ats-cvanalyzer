# backend/core_logic.py
import os
import time
from dotenv import load_dotenv
from typing import List, Dict, Union

# --- Imports ---
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
# V·∫™N D√ôNG HUGGING FACE CHO EMBEDDINGS (ƒê·ªÉ tr√°nh l·ªói API Google khi Embedding)
from langchain_huggingface import HuggingFaceEmbeddings 
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.runnables import RunnablePassthrough
from pydantic import BaseModel, Field

load_dotenv()

# --- DATA MODELS ---
class JobMatchResult(BaseModel):
    personal_info: Dict[str, str] = Field(description="Name, position, experience extracted from CV")
    matching_score: Dict[str, Union[int, str]] = Field(description="Percentage score and explanation")
    requirements_breakdown: Dict[str, str] = Field(description="Ratios for must-have and nice-to-have criteria")
    matched_keywords: List[str] = Field(description="List of matching technical skills")
    radar_chart: Dict[str, int] = Field(description="Scores 1-10 for 5 dimensions")
    bilingual_content: Dict[str, Union[Dict, List]] = Field(description="Assessment content in EN and VI")

CORE_PROMPT = """
B·∫°n l√† m·ªôt Tr·ª£ l√Ω Tuy·ªÉn d·ª•ng AI chuy√™n nghi·ªáp (JobMatchr). Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch CV (ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi d·∫°ng text) v√† M√¥ t·∫£ c√¥ng vi·ªác (JD - m·ªói d√≤ng l√† m·ªôt y√™u c·∫ßu).

**INPUT DATA:**
1. CV Text: {cv_text}
2. JD Text: {jd_text} (L∆∞u √Ω: M·ªói d√≤ng trong JD l√† m·ªôt ti√™u ch√≠ ri√™ng bi·ªát).

**NHI·ªÜM V·ª§:**
H√£y th·ª±c hi·ªán c√°c b∆∞·ªõc sau m·ªôt c√°ch logic:

B∆Ø·ªöC 1: TR√çCH XU·∫§T TH√îNG TIN C√Å NH√ÇN
- T√¨m Name, Position (V·ªã tr√≠ ·ª©ng tuy·ªÉn/hi·ªán t·∫°i), Experience (T·ªïng s·ªë nƒÉm kinh nghi·ªám - ch·ªâ l·∫•y s·ªë).

B∆Ø·ªöC 2: PH√ÇN T√çCH JD V√Ä T√çNH ƒêI·ªÇM (QUY T·∫ÆC "1 ƒê·ªÄU")
- T√°ch JD th√†nh c√°c d√≤ng ri√™ng bi·ªát. T·ªïng s·ªë d√≤ng = T·ªïng y√™u c·∫ßu (Total_Req).
- Ph√¢n lo·∫°i t·ª´ng d√≤ng th√†nh "B·∫Øt bu·ªôc" (Requirement) ho·∫∑c "∆Øu ti√™n" (Nice-to-have) d·ª±a tr√™n t·ª´ kh√≥a (n·∫øu kh√¥ng r√µ, m·∫∑c ƒë·ªãnh l√† B·∫Øt bu·ªôc).
- ƒê·ªëi chi·∫øu CV: V·ªõi m·ªói d√≤ng JD, n·∫øu CV c√≥ b·∫±ng ch·ª©ng ƒë√°p ·ª©ng => T√≠nh l√† 1 ƒëi·ªÉm (Matched).
- Keyword ph√°t hi·ªán: Tr√≠ch xu·∫•t c√°c t·ª´ kh√≥a k·ªπ thu·∫≠t (Hard skill) tr√πng kh·ªõp gi·ªØa CV v√† JD.
- C√¥ng th·ª©c t√≠nh % chung: (T·ªïng s·ªë d√≤ng Matched / T·ªïng s·ªë d√≤ng JD) * 100.

B∆Ø·ªöC 3: ƒê√ÅNH GI√Å SONG NG·ªÆ (ANH & VI·ªÜT)
- T·∫°o n·ªôi dung ƒë√°nh gi√° cho c√°c m·ª•c: ƒê√°nh gi√° chung, ƒêi·ªÉm m·∫°nh, ƒêi·ªÉm y·∫øu (Missing skills), C√¢u h·ªèi ph·ªèng v·∫•n.
- N·ªôi dung Ti·∫øng Anh vi·∫øt tr∆∞·ªõc, Ti·∫øng Vi·ªát d·ªãch s√°t nghƒ©a theo sau.

B∆Ø·ªöC 4: CH·∫§M ƒêI·ªÇM RADAR CHART (Thang 1-10)
- ƒê√°nh gi√° 5 kh√≠a c·∫°nh: Hard Skills, Soft Skills, Experience, Education, Domain Knowledge.

**OUTPUT FORMAT (B·∫ÆT BU·ªòC JSON):**
Ch·ªâ tr·∫£ v·ªÅ 1 JSON duy nh·∫•t, kh√¥ng c√≥ markdown, kh√¥ng c√≥ l·ªùi d·∫´n. C·∫•u tr√∫c nh∆∞ sau:
{{
    "personal_info": {{
        "name": "String",
        "position": "String (Single title only, e.g., 'Backend Developer')",
        "experience": "String (Single value only, e.g., '2 years')"
    }},
    "matching_score": {{
        "percentage": Integer,
        "explanation": "String (e.g., 'Matched 8/10 requirements')"
    }},
    "requirements_breakdown": {{
        "must_have_ratio": "String (e.g., '5/7')",
        "nice_to_have_ratio": "String (e.g., '3/3')"
    }},
    "matched_keywords": ["String", "String", ...],
    "radar_chart": {{
        "Hard Skills": Integer,
        "Soft Skills": Integer,
        "Experience": Integer,
        "Education": Integer,
        "Domain Knowledge": Integer
    }},
    "bilingual_content": {{
        "general_assessment": {{
            "en": "String",
            "vi": "String"
        }},
        "comparison_table": [
            {{
                "jd_requirement": "String (Original JD line)",
                "cv_evidence": "String (Evidence from CV or 'Not found')",
                "status": "Matched/Not Matched"
            }}
        ],
        "strengths": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }},
        "weaknesses_missing_skills": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }},
        "interview_questions": {{
            "en": ["String", "String"],
            "vi": ["String", "String"]
        }}
    }}
}}
"""

_llm_instance = None
_embedding_instance = None

def get_llm():
    global _llm_instance
    if _llm_instance is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            print("‚ùå L·ªñI NGHI√äM TR·ªåNG: Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY trong bi·∫øn m√¥i tr∆∞·ªùng!")
        else:
            print(f"‚úÖ ƒê√£ t√¨m th·∫•y API Key: {api_key[:5]}... (·∫©n ph·∫ßn sau)")
            
        # Quay l·∫°i d√πng gemini-flash-latest theo √Ω b·∫°n
        _llm_instance = ChatGoogleGenerativeAI(
            model="gemini-flash-latest", 
            temperature=0.2,
            google_api_key=api_key
        )
    return _llm_instance

def get_embeddings():
    global _embedding_instance
    if _embedding_instance is None:
        # D√πng Hugging Face (CPU) ƒë·ªÉ kh√¥ng c·∫ßn Key Google ·ªü b∆∞·ªõc n√†y
        _embedding_instance = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    return _embedding_instance

def analyze_cv_logic(file_path: str, jd_text: str):
    if not os.getenv("GOOGLE_API_KEY"):
        return {"error": "Server ch∆∞a nh·∫≠n ƒë∆∞·ª£c GOOGLE_API_KEY. H√£y ki·ªÉm tra Settings tr√™n Hugging Face."}

    # 1. X·ª≠ l√Ω PDF
    try:
        loader = PDFPlumberLoader(file_path)
        docs = loader.load()
        if not docs:
            return {"error": "Kh√¥ng th·ªÉ ƒë·ªçc n·ªôi dung t·ª´ file PDF."}
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)
    except Exception as e:
        return {"error": f"L·ªói ƒë·ªçc PDF: {str(e)}"}

    # 2. Vector Store & Chain
    try:
        embeddings = get_embeddings()
        llm = get_llm()

        # N·∫øu l·ªói x·∫£y ra ·ªü d√≤ng n√†y -> Code ch∆∞a c·∫≠p nh·∫≠t (v·∫´n d√πng Google Embeddings)
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            collection_name=f"cv_analysis_{int(time.time())}",
        )
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        parser = JsonOutputParser(pydantic_object=JobMatchResult)
        prompt = ChatPromptTemplate.from_template(CORE_PROMPT)
        prompt = prompt.partial(format_instructions=parser.get_format_instructions())

        def format_docs(docs):
            return "\n\n".join(d.page_content for d in docs)

        chain = (
            {"cv_text": retriever | format_docs, "jd_text": RunnablePassthrough()}
            | prompt
            | llm
            | parser
        )

        print("ü§ñ ƒêang ph√¢n t√≠ch v·ªõi Gemini Flash Latest...")
        result = chain.invoke(jd_text)
        
        vectorstore.delete_collection() 
        return result

    except Exception as e:
        # In l·ªói chi ti·∫øt ra console server ƒë·ªÉ debug
        print(f"‚ùå L·ªñI PH√ÇN T√çCH: {str(e)}")
        return {"error": f"L·ªói ph√¢n t√≠ch AI: {str(e)}"}
